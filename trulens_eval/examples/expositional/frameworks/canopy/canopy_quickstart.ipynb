{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruLens-Canopy Quickstart\n",
    "\n",
    " Canopy is an open-source framework and context engine built on top of the Pinecone vector database so you can build and host your own production-ready chat assistant at any scale. By integrating TruLens into your Canopy assistant, you can quickly iterate on and gain confidence in the quality of your chat assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qU canopy-sdk trulens-eval cohere ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"YOUR_PINECONE_API_KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "os.environ[\"CO_API_KEY\"] = \"YOUR_COHERE_API_KEY\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Downloading Pinecone's documentation as data to ingest to our Canopy chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>728aeea1-1dcf-5d0a-91f2-ecccd4dd4272</td>\n",
       "      <td># Scale indexes\\n\\n[Suggest Edits](/edit/scali...</td>\n",
       "      <td>https://docs.pinecone.io/docs/scaling-indexes</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'scaling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f19f269-171f-5556-93f3-a2d7eabbe50f</td>\n",
       "      <td># Understanding organizations\\n\\n[Suggest Edit...</td>\n",
       "      <td>https://docs.pinecone.io/docs/organizations</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b2a71cb3-5148-5090-86d5-7f4156edd7cf</td>\n",
       "      <td># Manage datasets\\n\\n[Suggest Edits](/edit/dat...</td>\n",
       "      <td>https://docs.pinecone.io/docs/datasets</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'datasets'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1dafe68a-2e78-57f7-a97a-93e043462196</td>\n",
       "      <td># Architecture\\n\\n[Suggest Edits](/edit/archit...</td>\n",
       "      <td>https://docs.pinecone.io/docs/architecture</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'archite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd</td>\n",
       "      <td># Moving to production\\n\\n[Suggest Edits](/edi...</td>\n",
       "      <td>https://docs.pinecone.io/docs/moving-to-produc...</td>\n",
       "      <td>{'created_at': '2023_10_25', 'title': 'moving-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id   \n",
       "0  728aeea1-1dcf-5d0a-91f2-ecccd4dd4272  \\\n",
       "1  2f19f269-171f-5556-93f3-a2d7eabbe50f   \n",
       "2  b2a71cb3-5148-5090-86d5-7f4156edd7cf   \n",
       "3  1dafe68a-2e78-57f7-a97a-93e043462196   \n",
       "4  8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd   \n",
       "\n",
       "                                                text   \n",
       "0  # Scale indexes\\n\\n[Suggest Edits](/edit/scali...  \\\n",
       "1  # Understanding organizations\\n\\n[Suggest Edit...   \n",
       "2  # Manage datasets\\n\\n[Suggest Edits](/edit/dat...   \n",
       "3  # Architecture\\n\\n[Suggest Edits](/edit/archit...   \n",
       "4  # Moving to production\\n\\n[Suggest Edits](/edi...   \n",
       "\n",
       "                                              source   \n",
       "0      https://docs.pinecone.io/docs/scaling-indexes  \\\n",
       "1        https://docs.pinecone.io/docs/organizations   \n",
       "2             https://docs.pinecone.io/docs/datasets   \n",
       "3         https://docs.pinecone.io/docs/architecture   \n",
       "4  https://docs.pinecone.io/docs/moving-to-produc...   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'created_at': '2023_10_25', 'title': 'scaling...  \n",
       "1  {'created_at': '2023_10_25', 'title': 'organiz...  \n",
       "2  {'created_at': '2023_10_25', 'title': 'datasets'}  \n",
       "3  {'created_at': '2023_10_25', 'title': 'archite...  \n",
       "4  {'created_at': '2023_10_25', 'title': 'moving-...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_parquet(\"https://storage.googleapis.com/pinecone-datasets-dev/pinecone_docs_ada-002/raw/file1.parquet\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Limits\n",
      "This is a summary of current Pinecone limitations. For many of these, there is a workaround or we're working on increasing the limits.\n",
      "\n",
      "## Upserts\n",
      "\n",
      "Max vector dimensionality is 20,000.\n",
      "\n",
      "Max size for an upsert request is 2MB. Recommended upsert limit is 100 vectors per request.\n",
      "\n",
      "Vectors may not be visible to queries immediately after upserting. You can check if the vectors were indexed by looking at the total with `describe_index_stats()`, although this method may not work if the index has multiple replicas. Pinecone is eventually consistent.\n",
      "\n",
      "Pinecone supports sparse vector values of sizes up to 1000 non-zero values.\n",
      "\n",
      "## Queries\n",
      "\n",
      "Max value for `top_k`, the number of results to return, is 10,000. Max value for `top_k` for queries with `include_metadata=True` or `include_data=True` is 1,000.\n",
      "\n",
      "......\n"
     ]
    }
   ],
   "source": [
    "print(data[\"text\"][50][:847].replace(\"\\n\\n\", \"\\n\").replace(\"[Suggest Edits](/edit/limits)\", \"\") + \"\\n......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' world', '!']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from canopy.tokenizer import Tokenizer\n",
    "Tokenizer.initialize()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.tokenize(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a812a3cc84444dbbb26fa8686daa7a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from canopy.knowledge_base import KnowledgeBase\n",
    "from canopy.models.data_models import Document\n",
    "from canopy.knowledge_base import list_canopy_indexes\n",
    "\n",
    "INDEX_NAME = \"my-index\"\n",
    "\n",
    "kb = KnowledgeBase(index_name=INDEX_NAME)\n",
    "\n",
    "if not any(name.endswith(INDEX_NAME) for name in list_canopy_indexes()):\n",
    "    kb.create_canopy_index()\n",
    "\n",
    "kb.connect()\n",
    "\n",
    "documents = [Document(**row) for _, row in data.iterrows()]\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    kb.upsert(documents[i: i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create context and chat engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from canopy.models.data_models import Query\n",
    "from canopy.context_engine import ContextEngine\n",
    "context_engine = ContextEngine(kb)\n",
    "\n",
    "from canopy.chat_engine import ChatEngine\n",
    "chat_engine = ChatEngine(context_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument static methods used by engine with TruLens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "\n",
    "from canopy.context_engine import ContextEngine\n",
    "instrument.method(ContextEngine, \"query\")\n",
    "\n",
    "from canopy.chat_engine import ChatEngine\n",
    "instrument.method(ChatEngine, \"chat\")\n",
    "\n",
    "from canopy.chat_engine.query_generator.base import QueryGenerator\n",
    "instrument.method(QueryGenerator, \"generate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feedback functions using instrumented methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru(database_redact_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.context_engine.query.rets.content.root[:].snippets[:].text.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.app.chat.rets.choices[0].message.content .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.app.chat.args.messages[0].content .\n",
      "✅ In Answer Relevance, input response will be set to __record__.app.chat.rets.choices[0].message.content .\n",
      "✅ In Context Relevance, input question will be set to __record__.app.chat.args.messages[0].content .\n",
      "✅ In Context Relevance, input statement will be set to __record__.app.context_engine.query.rets.content.root[:].snippets[:].text .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback, Select\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI as fOpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "fopenai = fOpenAI()\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=fopenai)\n",
    "\n",
    "intput = Select.RecordCalls.chat.args.messages[0].content\n",
    "context = Select.RecordCalls.context_engine.query.rets.content.root[:].snippets[:].text\n",
    "output = Select.RecordCalls.chat.rets.choices[0].message.content\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\", higher_is_better=True)\n",
    "    .on(context.collect())\n",
    "    .on(output)\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(fopenai.relevance_with_cot_reasons, name = \"Answer Relevance\", higher_is_better=True)\n",
    "    .on(intput)\n",
    "    .on(output)\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(fopenai.qs_relevance_with_cot_reasons, name = \"Context Relevance\", higher_is_better=True)\n",
    "    .on(intput)\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create recorded app and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function <function ChatEngine.chat at 0x16caa5e10> was not found during instrumentation walk. Make sure it is accessible by traversing app <canopy.chat_engine.chat_engine.ChatEngine object at 0x29d13cd00> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function QueryGenerator.generate at 0x16caa6320> was not found during instrumentation walk. Make sure it is accessible by traversing app <canopy.chat_engine.chat_engine.ChatEngine object at 0x29d13cd00> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function ContextEngine.query at 0x16caa6170> was not found during instrumentation walk. Make sure it is accessible by traversing app <canopy.chat_engine.chat_engine.ChatEngine object at 0x29d13cd00> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "\n",
    "app_id = \"canopy default\"\n",
    "tru_recorder = TruCustomApp(chat_engine, feedbacks = [f_groundedness, f_qa_relevance, f_context_relevance], app_id=app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsure what the main input string is for the call to chat with args [[UserMessage(role=<Role.USER: 'user'>, content='What is the max value for top_k pinecone supports?')]].\n",
      "Unsure what the main output string is for the call to chat with return type <class 'canopy.models.api_models.ChatResponse'>.\n",
      "Unsure what the main input string is for the call to chat with args [[UserMessage(role=<Role.USER: 'user'>, content='How can you get started with Pinecone and TruLens?')]].\n",
      "Unsure what the main output string is for the call to chat with return type <class 'canopy.models.api_models.ChatResponse'>.\n",
      "Unsure what the main input string is for the call to chat with args [[UserMessage(role=<Role.USER: 'user'>, content='What is the python command to upsert a list of vectors to Pinecone?')]].\n",
      "Unsure what the main output string is for the call to chat with return type <class 'canopy.models.api_models.ChatResponse'>.\n"
     ]
    }
   ],
   "source": [
    "from canopy.models.data_models import Messages, UserMessage\n",
    "\n",
    "queries = [\n",
    "    [UserMessage(content=\"What is the max value for top_k pinecone supports?\")],\n",
    "    [UserMessage(content=\"How can you get started with Pinecone and TruLens?\")],\n",
    "    [UserMessage(content=\"What is the python command to upsert a list of vectors to Pinecone?\")]\n",
    "]\n",
    "\n",
    "answers = []\n",
    "\n",
    "for query in queries:\n",
    "    with tru_recorder as recording:\n",
    "        response = chat_engine.chat(query)\n",
    "        answers.append(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the max value for top_k pinecone supports?\n",
      "\n",
      "The maximum value for the `top_k` parameter in Pinecone queries is 10,000. However, when using `top_k` with queries that have `include_metadata=True` or `include_data=True`, the maximum value is limited to 1,000. Source: https://docs.pinecone.io/docs/limits\n"
     ]
    }
   ],
   "source": [
    "print(queries[0][0].content + \"\\n\")\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "Dashboard already running at path:   Network URL: http://192.168.170.28:8501\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>canopy default</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.64963</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.003017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Groundedness  Answer Relevance  Context Relevance   latency   \n",
       "app_id                                                                        \n",
       "canopy default      0.555556          0.933333            0.64963  4.666667  \\\n",
       "\n",
       "                total_cost  \n",
       "app_id                      \n",
       "canopy default    0.003017  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[app_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Canopy with Cohere reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base.reranker.cohere import CohereReranker\n",
    "\n",
    "tru = Tru(database_redact_keys=True)\n",
    "\n",
    "INDEX_NAME = \"my-index\"\n",
    "\n",
    "kb = KnowledgeBase(index_name=INDEX_NAME, reranker=CohereReranker(top_n=5), default_top_k=20)\n",
    "kb.connect()\n",
    "\n",
    "context_engine = ContextEngine(kb)\n",
    "\n",
    "chat_engine = ChatEngine(context_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function <function ChatEngine.chat at 0x16caa5e10> was not found during instrumentation walk. Make sure it is accessible by traversing app <canopy.chat_engine.chat_engine.ChatEngine object at 0x290fd9810> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function QueryGenerator.generate at 0x16caa6320> was not found during instrumentation walk. Make sure it is accessible by traversing app <canopy.chat_engine.chat_engine.ChatEngine object at 0x290fd9810> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function ContextEngine.query at 0x16caa6170> was not found during instrumentation walk. Make sure it is accessible by traversing app <canopy.chat_engine.chat_engine.ChatEngine object at 0x290fd9810> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Unsure what the main input string is for the call to chat with args [[UserMessage(role=<Role.USER: 'user'>, content='What is the max value for top_k pinecone supports?')]].\n",
      "Unsure what the main output string is for the call to chat with return type <class 'canopy.models.api_models.ChatResponse'>.\n",
      "Unsure what the main input string is for the call to chat with args [[UserMessage(role=<Role.USER: 'user'>, content='How can you get started with Pinecone and TruLens?')]].\n",
      "Unsure what the main output string is for the call to chat with return type <class 'canopy.models.api_models.ChatResponse'>.\n",
      "Unsure what the main input string is for the call to chat with args [[UserMessage(role=<Role.USER: 'user'>, content='What is the python command to upsert a list of vectors to Pinecone?')]].\n",
      "Unsure what the main output string is for the call to chat with return type <class 'canopy.models.api_models.ChatResponse'>.\n"
     ]
    }
   ],
   "source": [
    "reranking_app_id = \"canopy_reranking\"\n",
    "reranking_tru_recorder = TruCustomApp(chat_engine, feedbacks = [f_groundedness, f_qa_relevance, f_context_relevance], app_id=reranking_app_id)\n",
    "\n",
    "for query in queries:\n",
    "    with reranking_tru_recorder as recording:\n",
    "        response = chat_engine.chat(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the effect of reranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>canopy_reranking</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.794286</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canopy default</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.649630</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.003017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Groundedness  Answer Relevance  Context Relevance   latency   \n",
       "app_id                                                                          \n",
       "canopy_reranking      1.000000          0.933333           0.794286  4.666667  \\\n",
       "canopy default        0.555556          0.933333           0.649630  4.666667   \n",
       "\n",
       "                  total_cost  \n",
       "app_id                        \n",
       "canopy_reranking    0.003100  \n",
       "canopy default      0.003017  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[app_id, reranking_app_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b49ef6d0b3ca0fd6117ebbca48c3d697c422d5d25bd8bdbbbbafb3db0f51ca63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
